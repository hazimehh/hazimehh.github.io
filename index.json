[{"authors":["admin"],"categories":null,"content":"About Me Welcome to my page! I am a research scientist at Google in New York. My research is broadly on machine learning (ML) and optimization. Recent ML topics I have been working on include:\n Sparsity and interpretability Out-of-distribution generalization and robustness Multitask learning  I am also involved in applications of the topics above to forecasting, trust and safety, and autonomous systems. My work has been recognized with several best paper awards from SIGKDD, INFORMS, and MIT.\nI completed my PhD at MIT under the supervision of Rahul Mazumder, where I worked on highly scalable optimization algorithms for variable selection in machine learning. Before that, I did my masters at UIUC where I worked with ChengXiang Zhai on improving information recall in search engines.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://hazimehh.github.io/author/hussein-hazimeh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hussein-hazimeh/","section":"authors","summary":"About Me Welcome to my page! I am a research scientist at Google in New York. My research is broadly on machine learning (ML) and optimization. Recent ML topics I have been working on include:","tags":null,"title":"Hussein Hazimeh","type":"authors"},{"authors":null,"categories":null,"content":"Citation of the INFORMS Optimization Society Young Researchers Prize Hussein Hazimeh and Rahul Mazumder are awarded the 2020 INFORMS Optimization Society Prize for Young Researchers for their paper, \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms,\u0026rdquo; Operations Research, to appear. The paper presents a novel algorithm for the Best Subset Selection problem (BSS), which is ordinary least-squares linear regression but with a penalty (or constraint) on the number of non-zero coefficients in the model, which induces sparsity. BSS is a fundamental problem in high-dimensional statistics, where sparse solutions with good explanatory power are crucial for practical use. Yet, it is NP-hard and, therefore, has been labeled computationally intractable and replaced by popular formulations such as LASSO and ridge regression. The authors combine tools and techniques from high-dimensional statistics, continuous optimization, integer programming, and open-source software development to provide the community with a highly effective heuristic for BSS, one which has strong theoretical underpinnings, outperforms existing state-of-the-art codes in many cases, and is easy to generalize to other settings.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8252baf8dc0ec6f81d26075c02c4b567","permalink":"http://hazimehh.github.io/researcher-award/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/researcher-award/","section":"","summary":"Citation of the INFORMS Optimization Society Young Researchers Prize Hussein Hazimeh and Rahul Mazumder are awarded the 2020 INFORMS Optimization Society Prize for Young Researchers for their paper, \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms,\u0026rdquo; Operations Research, to appear.","tags":null,"title":"Award Citation","type":"page"},{"authors":null,"categories":null,"content":"Selected Awards   Best Student Paper Award \u0026ndash; SIGKDD (2022). Link.\nReceived as an advisor to the student first author.\nPaper: Flexible Modeling and Multitask Learning using Differentiable Tree Ensembles\n  Young Researchers Prize \u0026ndash; INFORMS Optimization Society (2020). Citation. Link.\nReceived \u0026ldquo;for an outstanding paper in optimization\u0026rdquo;.\nPaper: Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms\n  Best Paper Award \u0026ndash; MIT Operations Research Center (2020).\nPaper: Sparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization\n  Honorable Mention \u0026ndash; INFORMS Computing Society (2020). Link.\n  Honorable Mention \u0026ndash; Mixed Integer Programming Workshop (2019).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bc0e0b6c7f346947a35f12615368b3d4","permalink":"http://hazimehh.github.io/awards/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/awards/","section":"","summary":"Selected Awards   Best Student Paper Award \u0026ndash; SIGKDD (2022). Link.\nReceived as an advisor to the student first author.\nPaper: Flexible Modeling and Multitask Learning using Differentiable Tree Ensembles","tags":null,"title":"Awards","type":"page"},{"authors":null,"categories":null,"content":"Selected Publications ** This page is outdated. Please see my Google scholar page for a full list including recent work.**\nGrouped Variable Selection using Discrete Optimization. Link.\nHussein Hazimeh, Rahul Mazumder, and Peter Radchenko. Annals of Statistics (2023+).\nSparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization. Link. Code.\nHussein Hazimeh, Rahul Mazumder, and Ali Saab. Mathematical Programming (2022).\n Best Student Paper Award, MIT ORC.\n Honorable Mention, INFORMS Computing Society.\nFast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms. Link. Code.\nHussein Hazimeh and Rahul Mazumder. Operations Research (2020).\n Young Researchers Prize (for an Outstanding Paper in Optimization), INFORMS Optimization Society.\nLearning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives. Link. Code.\nAntoine Dedieu, Hussein Hazimeh, and Rahul Mazumder. JMLR (2021, to appear)\nThe Tree Ensemble Layer: Differentiability meets Conditional Computation. Link. Code.\nHussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. ICML (2020).\nLearning Hierarchical Interactions at Scale: A Convex Optimization Approach. Link. Code.\nHussein Hazimeh and Rahul Mazumder. AISTATS (2020).\nAxiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback. Link.\nHussein Hazimeh and ChengXiang Zhai. SIGIR ICTIR (2015).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"480c4de99851329b51acecc000e2e84f","permalink":"http://hazimehh.github.io/publications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publications/","section":"","summary":"Selected Publications ** This page is outdated. Please see my Google scholar page for a full list including recent work.**\nGrouped Variable Selection using Discrete Optimization. Link.\nHussein Hazimeh, Rahul Mazumder, and Peter Radchenko.","tags":null,"title":"Publications","type":"page"},{"authors":null,"categories":null,"content":"Software I am the main developer of the following software toolkits for interpretable machine learning:\n L0Learn: Fast, approximate algorithms for L0-regularized learning. Github. CRAN. Vignette.\nHandles sparse regression and classification problems, in times comparable to the fast Lasso solvers.\nDownloaded over 38,000 times.\nBased on \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms\u0026rdquo;. Link.  L0BnB: A specialized exact algorithm for L0-regularized regression, based on branch-and-bound. Github. PyPI.\nCan solve instances with 10^7 features to optimality, in minutes to hours (assuming highly sparse solutions).\nDownloaded over 9,000 times.\nBased on \u0026ldquo;Sparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization\u0026rdquo;. Link.  hierScale: A scalable convex optimization algorithm for regression with pairwise interactions. Github. PyPI.\nBased on \u0026ldquo;Learning Hierarchical Interactions at Scale: A Convex Optimization Approach\u0026rdquo;. Link.  Tree Ensemble Layer: A layer of differentiable trees that can be used within neural networks. Github.\nThe layer can speed up and enhance interpretability in neural networks.\nJoint work with Google collaborators.\nBased on \u0026ldquo;The Tree Ensemble Layer: Differentiability meets Conditional Computation\u0026rdquo;. Link.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2f393b616dd89a2e372fd85789b1e963","permalink":"http://hazimehh.github.io/software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/software/","section":"","summary":"Software I am the main developer of the following software toolkits for interpretable machine learning:\n L0Learn: Fast, approximate algorithms for L0-regularized learning. Github. CRAN. Vignette.\nHandles sparse regression and classification problems, in times comparable to the fast Lasso solvers.","tags":null,"title":"Software","type":"page"},{"authors":null,"categories":null,"content":"Recent Talks Sparse Regression at Scale\nMIT, ORC Seminar, 2020. Link.\nDiscrete Optimization Talks, 2020. Video.\nINFORMS Annual Meeting, 2020. Video.\nBernoulli-IMS One World Symposium, 2020.\nGoogle Brain, 2020.\nFast Best Subset Selection\n\u000f INFORMS Annual Meeting (IOS Society Prize Session), 2020. Video.\n\u000f Google Research, 2019.\n\u000f CMStatistics, 2017.\nSmooth Routing in Neural Networks\n\u000f Google Brain, 2020.\n\u000f Google PIRC Spotlight Talk, 2020.\nThe Tree Ensemble Layer: Differentiability meets Conditional Computation\n\u000f ICML, 2020. Video.\nGoogle Research, 2019.\nLearning Hierarchical Interactions at Scale\n\u000f AISTATS, 2020. Video.\n\u000f INFORMS Annual Meeting, 2019.\n\u000f Google Research, 2019.\n\u000f Rensselaer Polytechnic Institute (Applied Math Days), 2019.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"292abb77c6d67b63273922561146b1c5","permalink":"http://hazimehh.github.io/talks/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/talks/","section":"talks","summary":"Recent Talks Sparse Regression at Scale\nMIT, ORC Seminar, 2020. Link.\nDiscrete Optimization Talks, 2020. Video.\nINFORMS Annual Meeting, 2020. Video.\nBernoulli-IMS One World Symposium, 2020.\nGoogle Brain, 2020.\nFast Best Subset Selection","tags":null,"title":"Talks","type":"talks"}]