[{"authors":["admin"],"categories":null,"content":"About me Welcome to my page! I\u0026rsquo;m a senior research scientist at Google in New York. My research is broadly on deep learning and statistical learning. I currently lead an effort on efficiently predicting data value in foundation models (with applications to Gemini and Gemma) and research fundamental techniques for model compression. I\u0026rsquo;ve also been managing Google-sponsored research collaborations with universities for the past three years.\nMy research has been recognized with best paper awards and honorable mentions from INFORMS ICS (2023), KDD (2022), INFORMS IOS (2020), INFORMS ICS (2020), MIT (2020), MIP Workshop (2019).\nI completed my PhD at MIT where I was advised by Rahul Mazumder and worked on scalable algorithms for sparse learning. Before that, I did my masters at UIUC where I worked with ChengXiang Zhai on improving information recall in search engines.\nCurrent research  Data value in foundation models  Designing efficient methods for predicting the impact of any training data mixture on the performance of a given model (without training) Researching principled statistical methods to improve the design of data ablations Our approaches are being used in the development of Gemini and Gemma   Model compression and efficiency  Pruning transformer-based foundation models (focus on reducing latency) Improving routing in mixture of experts Feature selection (best subset selection) in standard statistical learning    ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.hazimeh.com/author/hussein-hazimeh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hussein-hazimeh/","section":"authors","summary":"About me Welcome to my page! I\u0026rsquo;m a senior research scientist at Google in New York. My research is broadly on deep learning and statistical learning. I currently lead an effort on efficiently predicting data value in foundation models (with applications to Gemini and Gemma) and research fundamental techniques for model compression.","tags":null,"title":"Hussein Hazimeh","type":"authors"},{"authors":null,"categories":null,"content":"Citation of the INFORMS Optimization Society Young Researchers Prize Hussein Hazimeh and Rahul Mazumder are awarded the 2020 INFORMS Optimization Society Prize for Young Researchers for their paper, \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms,\u0026rdquo; Operations Research, to appear. The paper presents a novel algorithm for the Best Subset Selection problem (BSS), which is ordinary least-squares linear regression but with a penalty (or constraint) on the number of non-zero coefficients in the model, which induces sparsity. BSS is a fundamental problem in high-dimensional statistics, where sparse solutions with good explanatory power are crucial for practical use. Yet, it is NP-hard and, therefore, has been labeled computationally intractable and replaced by popular formulations such as LASSO and ridge regression. The authors combine tools and techniques from high-dimensional statistics, continuous optimization, integer programming, and open-source software development to provide the community with a highly effective heuristic for BSS, one which has strong theoretical underpinnings, outperforms existing state-of-the-art codes in many cases, and is easy to generalize to other settings.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8252baf8dc0ec6f81d26075c02c4b567","permalink":"https://www.hazimeh.com/researcher-award/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/researcher-award/","section":"","summary":"Citation of the INFORMS Optimization Society Young Researchers Prize Hussein Hazimeh and Rahul Mazumder are awarded the 2020 INFORMS Optimization Society Prize for Young Researchers for their paper, \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms,\u0026rdquo; Operations Research, to appear.","tags":null,"title":"Award Citation","type":"page"},{"authors":null,"categories":null,"content":"Selected Awards   Honorable Mention \u0026ndash; INFORMS ICS Prize (2023). Link.\nReceived for a body of work on sparse learning.\n  Best Student Paper Award \u0026ndash; SIGKDD (2022). Link.\nReceived as an advisor to the student first author.\nPaper: Flexible Modeling and Multitask Learning using Differentiable Tree Ensembles\n  Young Researchers Prize \u0026ndash; INFORMS Optimization Society (2020). Citation. Link.\nReceived \u0026ldquo;for an outstanding paper in optimization\u0026rdquo;.\nPaper: Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms\n  Best Paper Award \u0026ndash; MIT Operations Research Center (2020).\nPaper: Sparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization\n  Honorable Mention \u0026ndash; INFORMS Computing Society (2020). Link.\n  Honorable Mention \u0026ndash; Mixed Integer Programming Workshop (2019).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bc0e0b6c7f346947a35f12615368b3d4","permalink":"https://www.hazimeh.com/awards/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/awards/","section":"","summary":"Selected Awards   Honorable Mention \u0026ndash; INFORMS ICS Prize (2023). Link.\nReceived for a body of work on sparse learning.\n  Best Student Paper Award \u0026ndash; SIGKDD (2022). Link.","tags":null,"title":"Awards","type":"page"},{"authors":null,"categories":null,"content":"Featured Publications  Fast as CHITA: Neural Network Pruning with Combinatorial Optimization. Paper. Blog.\nRiade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, Rahul Mazumder. ICML (2023).   Pruning as a best subset selection problem.   How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy. Paper. Blog.\nNatalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien, Abhradeep Thakurta. JAIR (2023).   DP-SGD for training differentially private neural nets.  Selected Publications ** The list below does not include all publications and may be outdated. Please see my Google scholar page for a full list including recent work.**\nBenchmarking Robustness to Adversarial Image Obfuscations. Link.\nFlorian Stimberg, Ayan Chakrabarti, Chun-Ta Lu, Hussein Hazimeh, Otilia Stretcu, Wei Qiao, Yintao Liu, Merve Kaya, Cyrus Rashtchian, Ariel Fuxman, Mehmet Tek, Sven Gowal. NeurIPS (2023).\nL0learn: A scalable package for sparse learning using l0 regularization. Link.\nHussein Hazimeh, Rahul Mazumder, and Tim Nonet. JMLR (2023).\nGrouped Variable Selection with Discrete Optimization: Computational and Statistical Perspectives. Link.\nHussein Hazimeh, Rahul Mazumder, and Peter Radchenko. Annals of Statistics (2023).\nSparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization. Link. Code.\nHussein Hazimeh, Rahul Mazumder, and Ali Saab. Mathematical Programming (2022).\n Best Student Paper Award, MIT ORC.\n Honorable Mention, INFORMS Computing Society.\nFlexible Modeling and Multitask Learning using Differentiable Tree Ensembles. Link.\nShibal Ibrahim, Hussein Hazimeh, and Rahul Mazumder. SIGKDD (2022).\n Best Student Paper Award, SIGKDD (2022).\nDselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Link.\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder\nLichan Hong, and Ed Chi. NeurIPS (2021).\nLearning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives. Link. Code.\nAntoine Dedieu, Hussein Hazimeh, and Rahul Mazumder. JMLR (2021).\nFast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms. Link. Code.\nHussein Hazimeh and Rahul Mazumder. Operations Research (2020).\n Young Researchers Prize (for an Outstanding Paper in Optimization), INFORMS Optimization Society.\nThe Tree Ensemble Layer: Differentiability meets Conditional Computation. Link. Code.\nHussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. ICML (2020).\nLearning Hierarchical Interactions at Scale: A Convex Optimization Approach. Link. Code.\nHussein Hazimeh and Rahul Mazumder. AISTATS (2020).\nAxiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback. Link.\nHussein Hazimeh and ChengXiang Zhai. SIGIR ICTIR (2015).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"480c4de99851329b51acecc000e2e84f","permalink":"https://www.hazimeh.com/publications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publications/","section":"","summary":"Featured Publications  Fast as CHITA: Neural Network Pruning with Combinatorial Optimization. Paper. Blog.\nRiade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, Rahul Mazumder. ICML (2023).   Pruning as a best subset selection problem.","tags":null,"title":"Publications","type":"page"},{"authors":null,"categories":null,"content":"Software I am the main developer of the following software toolkits for interpretable machine learning:\n L0Learn: Fast, approximate algorithms for L0-regularized learning. Github. CRAN. Vignette.\nHandles sparse regression and classification problems, in times comparable to the fast Lasso solvers.\nDownloaded over 38,000 times.\nBased on \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms\u0026rdquo;. Link.  L0BnB: A specialized exact algorithm for L0-regularized regression, based on branch-and-bound. Github. PyPI.\nCan solve instances with 10^7 features to optimality, in minutes to hours (assuming highly sparse solutions).\nDownloaded over 9,000 times.\nBased on \u0026ldquo;Sparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization\u0026rdquo;. Link.  hierScale: A scalable convex optimization algorithm for regression with pairwise interactions. Github. PyPI.\nBased on \u0026ldquo;Learning Hierarchical Interactions at Scale: A Convex Optimization Approach\u0026rdquo;. Link.  Tree Ensemble Layer: A layer of differentiable trees that can be used within neural networks. Github.\nThe layer can speed up and enhance interpretability in neural networks.\nJoint work with Google collaborators.\nBased on \u0026ldquo;The Tree Ensemble Layer: Differentiability meets Conditional Computation\u0026rdquo;. Link.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2f393b616dd89a2e372fd85789b1e963","permalink":"https://www.hazimeh.com/software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/software/","section":"","summary":"Software I am the main developer of the following software toolkits for interpretable machine learning:\n L0Learn: Fast, approximate algorithms for L0-regularized learning. Github. CRAN. Vignette.\nHandles sparse regression and classification problems, in times comparable to the fast Lasso solvers.","tags":null,"title":"Software","type":"page"}]