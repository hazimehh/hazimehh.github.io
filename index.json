[{"authors":["admin"],"categories":null,"content":"About Me Welcome to my page! I am a research scientist at Google in New York. My research is broadly on machine learning and optimization. Some topics I have been recently working on:\n Sparsity and interpretability in machine learning Out-of-distribution generalization and robustness Multitask learning  I have also been involved in applications of the above to forecasting, trust and safety, and autonomous systems. My research has been recognized with several best paper awards from SIGKDD, INFORMS, and MIT.\nI completed my PhD at MIT where I was advised by Rahul Mazumder and worked on scalable algorithms for sparse learning. Before that, I did my masters at UIUC where I worked with ChengXiang Zhai on improving information recall in search engines.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://hazimehh.github.io/author/hussein-hazimeh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hussein-hazimeh/","section":"authors","summary":"About Me Welcome to my page! I am a research scientist at Google in New York. My research is broadly on machine learning and optimization. Some topics I have been recently working on:","tags":null,"title":"Hussein Hazimeh","type":"authors"},{"authors":null,"categories":null,"content":"Citation of the INFORMS Optimization Society Young Researchers Prize Hussein Hazimeh and Rahul Mazumder are awarded the 2020 INFORMS Optimization Society Prize for Young Researchers for their paper, \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms,\u0026rdquo; Operations Research, to appear. The paper presents a novel algorithm for the Best Subset Selection problem (BSS), which is ordinary least-squares linear regression but with a penalty (or constraint) on the number of non-zero coefficients in the model, which induces sparsity. BSS is a fundamental problem in high-dimensional statistics, where sparse solutions with good explanatory power are crucial for practical use. Yet, it is NP-hard and, therefore, has been labeled computationally intractable and replaced by popular formulations such as LASSO and ridge regression. The authors combine tools and techniques from high-dimensional statistics, continuous optimization, integer programming, and open-source software development to provide the community with a highly effective heuristic for BSS, one which has strong theoretical underpinnings, outperforms existing state-of-the-art codes in many cases, and is easy to generalize to other settings.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8252baf8dc0ec6f81d26075c02c4b567","permalink":"http://hazimehh.github.io/researcher-award/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/researcher-award/","section":"","summary":"Citation of the INFORMS Optimization Society Young Researchers Prize Hussein Hazimeh and Rahul Mazumder are awarded the 2020 INFORMS Optimization Society Prize for Young Researchers for their paper, \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms,\u0026rdquo; Operations Research, to appear.","tags":null,"title":"Award Citation","type":"page"},{"authors":null,"categories":null,"content":"Selected Awards   Best Student Paper Award \u0026ndash; SIGKDD (2022). Link.\nReceived as an advisor to the student first author.\nPaper: Flexible Modeling and Multitask Learning using Differentiable Tree Ensembles\n  Young Researchers Prize \u0026ndash; INFORMS Optimization Society (2020). Citation. Link.\nReceived \u0026ldquo;for an outstanding paper in optimization\u0026rdquo;.\nPaper: Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms\n  Best Paper Award \u0026ndash; MIT Operations Research Center (2020).\nPaper: Sparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization\n  Honorable Mention \u0026ndash; INFORMS Computing Society (2020). Link.\n  Honorable Mention \u0026ndash; Mixed Integer Programming Workshop (2019).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bc0e0b6c7f346947a35f12615368b3d4","permalink":"http://hazimehh.github.io/awards/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/awards/","section":"","summary":"Selected Awards   Best Student Paper Award \u0026ndash; SIGKDD (2022). Link.\nReceived as an advisor to the student first author.\nPaper: Flexible Modeling and Multitask Learning using Differentiable Tree Ensembles","tags":null,"title":"Awards","type":"page"},{"authors":null,"categories":null,"content":"Selected Publications ** This page is outdated. Please see my Google scholar page for a full list including recent work.**\nGrouped Variable Selection with Discrete Optimization: Computational and Statistical Perspectives. Link.\nHussein Hazimeh, Rahul Mazumder, and Peter Radchenko. Annals of Statistics (2023+).\nSparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization. Link. Code.\nHussein Hazimeh, Rahul Mazumder, and Ali Saab. Mathematical Programming (2022).\n Best Student Paper Award, MIT ORC.\n Honorable Mention, INFORMS Computing Society.\nFlexible Modeling and Multitask Learning using Differentiable Tree Ensembles. Link.\nShibal Ibrahim, Hussein Hazimeh, and Rahul Mazumder. SIGKDD (2022).\n Best Student Paper Award, SIGKDD (2022).\nDselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Link.\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder\nLichan Hong, and Ed Chi. NeurIPS (2021).\nLearning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives. Link. Code.\nAntoine Dedieu, Hussein Hazimeh, and Rahul Mazumder. JMLR (2021).\nFast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms. Link. Code.\nHussein Hazimeh and Rahul Mazumder. Operations Research (2020).\n Young Researchers Prize (for an Outstanding Paper in Optimization), INFORMS Optimization Society.\nThe Tree Ensemble Layer: Differentiability meets Conditional Computation. Link. Code.\nHussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. ICML (2020).\nLearning Hierarchical Interactions at Scale: A Convex Optimization Approach. Link. Code.\nHussein Hazimeh and Rahul Mazumder. AISTATS (2020).\nAxiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback. Link.\nHussein Hazimeh and ChengXiang Zhai. SIGIR ICTIR (2015).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"480c4de99851329b51acecc000e2e84f","permalink":"http://hazimehh.github.io/publications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publications/","section":"","summary":"Selected Publications ** This page is outdated. Please see my Google scholar page for a full list including recent work.**\nGrouped Variable Selection with Discrete Optimization: Computational and Statistical Perspectives. Link.","tags":null,"title":"Publications","type":"page"},{"authors":null,"categories":null,"content":"Software I am the main developer of the following software toolkits for interpretable machine learning:\n L0Learn: Fast, approximate algorithms for L0-regularized learning. Github. CRAN. Vignette.\nHandles sparse regression and classification problems, in times comparable to the fast Lasso solvers.\nDownloaded over 38,000 times.\nBased on \u0026ldquo;Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms\u0026rdquo;. Link.  L0BnB: A specialized exact algorithm for L0-regularized regression, based on branch-and-bound. Github. PyPI.\nCan solve instances with 10^7 features to optimality, in minutes to hours (assuming highly sparse solutions).\nDownloaded over 9,000 times.\nBased on \u0026ldquo;Sparse Regression at Scale: Branch-and-Bound rooted in First-Order Optimization\u0026rdquo;. Link.  hierScale: A scalable convex optimization algorithm for regression with pairwise interactions. Github. PyPI.\nBased on \u0026ldquo;Learning Hierarchical Interactions at Scale: A Convex Optimization Approach\u0026rdquo;. Link.  Tree Ensemble Layer: A layer of differentiable trees that can be used within neural networks. Github.\nThe layer can speed up and enhance interpretability in neural networks.\nJoint work with Google collaborators.\nBased on \u0026ldquo;The Tree Ensemble Layer: Differentiability meets Conditional Computation\u0026rdquo;. Link.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2f393b616dd89a2e372fd85789b1e963","permalink":"http://hazimehh.github.io/software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/software/","section":"","summary":"Software I am the main developer of the following software toolkits for interpretable machine learning:\n L0Learn: Fast, approximate algorithms for L0-regularized learning. Github. CRAN. Vignette.\nHandles sparse regression and classification problems, in times comparable to the fast Lasso solvers.","tags":null,"title":"Software","type":"page"}]